#!/usr/bin/env python3
"""
ä¸ºæ¯å¼ å›¾ç‰‡ç”Ÿæˆè¯¦ç»†çš„ä¸ªäººåˆ†ææŠ¥å‘Š
"""

import argparse
import json
from pathlib import Path
import sys
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.demo import load_image, preprocess_eye_image, predict
from src.data.unified_extractor import create_eye_extractor
from src.models.eye_iq_net import EyeIQNet
import cv2
import torch


def generate_report(image_path: Path, checkpoint_path: Path, output_dir: Path, 
                   skip_extraction: bool = False, device: str = "cuda"):
    """ä¸ºå•å¼ å›¾ç‰‡ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
    
    # åŠ è½½å›¾åƒ
    print(f"æ­£åœ¨åˆ†æ: {image_path.name}")
    image = load_image(image_path)
    
    # æå–çœ¼éƒ¨åŒºåŸŸ
    if skip_extraction:
        eye_image = image
        if len(eye_image.shape) == 3 and eye_image.shape[2] == 3:
            eye_image = cv2.cvtColor(eye_image, cv2.COLOR_BGR2RGB)
            eye_image = cv2.cvtColor(eye_image, cv2.COLOR_RGB2BGR)
    else:
        try:
            extractor = create_eye_extractor(backend="auto", image_size=224, device=device)
            eye_image = extractor.extract_eye_region(image, eye_side="both")
            if eye_image is None:
                print(f"è­¦å‘Š: æ— æ³•æå–çœ¼éƒ¨åŒºåŸŸï¼Œä½¿ç”¨åŸå›¾")
                eye_image = image
        except:
            eye_image = image
    
    # é¢„å¤„ç†
    image_tensor = preprocess_eye_image(eye_image)
    
    # åŠ è½½æ¨¡å‹
    device_obj = torch.device(device)
    model = EyeIQNet(num_emotions=7, pretrained=True)
    
    if checkpoint_path.exists():
        checkpoint = torch.load(checkpoint_path, map_location=device_obj)
        if "model_state" in checkpoint:
            model.load_state_dict(checkpoint["model_state"])
        else:
            model.load_state_dict(checkpoint)
    
    model.to(device_obj)
    
    # é¢„æµ‹
    emotion_labels = ["æ„¤æ€’", "åŒæ¶", "ææƒ§", "å¿«ä¹", "ä¸­æ€§", "æ‚²ä¼¤", "æƒŠè®¶"]
    result = predict(model, image_tensor, device_obj, emotion_labels)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = generate_markdown_report(image_path, result, eye_image.shape)
    
    # ä¿å­˜æŠ¥å‘Š
    output_dir.mkdir(parents=True, exist_ok=True)
    report_path = output_dir / f"{image_path.stem}_report.md"
    report_path.write_text(report, encoding='utf-8')
    
    # ä¿å­˜JSONæ ¼å¼
    json_path = output_dir / f"{image_path.stem}_report.json"
    json_data = {
        "image_name": image_path.name,
        "analysis_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "results": result
    }
    json_path.write_text(json.dumps(json_data, indent=2, ensure_ascii=False), encoding='utf-8')
    
    print(f"âœ“ æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    return report_path, json_path


def generate_markdown_report(image_path: Path, result: dict, image_shape: tuple) -> str:
    """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
    
    emotion_data = result['æƒ…ç»ªé¢„æµ‹']
    dimension_data = result['æƒ…ç»ªç»´åº¦']
    ability_data = result['èƒ½åŠ›è¯„ä¼°']
    raw_data = result['åŸå§‹è¾“å‡º']
    
    # æƒ…ç»ªæ¦‚ç‡æ’åº
    emotion_probs = []
    for emotion, prob_str in emotion_data['æ‰€æœ‰æƒ…ç»ªæ¦‚ç‡'].items():
        prob = float(prob_str.replace('%', ''))
        emotion_probs.append((emotion, prob))
    emotion_probs.sort(key=lambda x: x[1], reverse=True)
    
    # ç”Ÿæˆæƒ…ç»ªæ¡å½¢å›¾ï¼ˆæ–‡æœ¬ï¼‰
    emotion_bars = ""
    for emotion, prob in emotion_probs:
        bar_length = int(prob / 2)  # æ¯2%ä¸€ä¸ªå­—ç¬¦
        bar = "â–ˆ" * bar_length + "â–‘" * (50 - bar_length)
        emotion_bars += f"{emotion:6s} â”‚{bar}â”‚ {prob:5.2f}%\n"
    
    # èƒ½åŠ›è¯„ä¼°è§£è¯»
    iq_score = float(ability_data['IQ ä»£ç†åˆ†æ•°'].split('/')[0])
    eq_score = float(ability_data['EQ ä»£ç†åˆ†æ•°'].split('/')[0])
    
    iq_level = get_score_level(iq_score)
    eq_level = get_score_level(eq_score)
    
    # æƒ…ç»ªç»´åº¦è§£è¯»
    valence = float(dimension_data['æ•ˆä»· (Valence)'])
    arousal = float(dimension_data['å”¤é†’åº¦ (Arousal)'])
    
    valence_desc = "ç§¯æ" if valence > 0.3 else "ä¸­æ€§" if valence > -0.3 else "æ¶ˆæ"
    arousal_desc = "é«˜å”¤é†’" if arousal > 0.3 else "ä¸­ç­‰å”¤é†’" if arousal > -0.3 else "ä½å”¤é†’"
    
    report = f"""# çœ¼éƒ¨æƒ…ç»ªä¸èƒ½åŠ›åˆ†ææŠ¥å‘Š

**å›¾ç‰‡åç§°**: {image_path.name}  
**åˆ†ææ—¶é—´**: {datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")}  
**å›¾ç‰‡å°ºå¯¸**: {image_shape[1]} Ã— {image_shape[0]} åƒç´ 

---

## ğŸ“Š æƒ…ç»ªåˆ†æ

### ä¸»è¦æƒ…ç»ªè¯†åˆ«

**æ£€æµ‹åˆ°çš„æƒ…ç»ª**: **{emotion_data['ä¸»è¦æƒ…ç»ª']}**  
**ç½®ä¿¡åº¦**: {emotion_data['ç½®ä¿¡åº¦']}

### è¯¦ç»†æƒ…ç»ªåˆ†å¸ƒ

```
{emotion_bars}
```

### æƒ…ç»ªç»´åº¦åˆ†æ

- **æ•ˆä»· (Valence)**: {dimension_data['æ•ˆä»· (Valence)']}
  - è§£è¯»: {valence_desc}ï¼ˆèŒƒå›´: -1 åˆ° 1ï¼Œæ­£å€¼è¡¨ç¤ºç§¯ææƒ…ç»ªï¼‰
  
- **å”¤é†’åº¦ (Arousal)**: {dimension_data['å”¤é†’åº¦ (Arousal)']}
  - è§£è¯»: {arousal_desc}ï¼ˆèŒƒå›´: -1 åˆ° 1ï¼Œæ­£å€¼è¡¨ç¤ºé«˜å”¤é†’çŠ¶æ€ï¼‰

### æƒ…ç»ªç‰¹å¾æ€»ç»“

æ ¹æ®åˆ†æç»“æœï¼Œè¯¥çœ¼éƒ¨å›¾åƒè¡¨ç°å‡ºä»¥ä¸‹æƒ…ç»ªç‰¹å¾ï¼š
- ä¸»è¦æƒ…ç»ªä¸º **{emotion_data['ä¸»è¦æƒ…ç»ª']}**ï¼Œç½®ä¿¡åº¦è¾¾åˆ° {emotion_data['ç½®ä¿¡åº¦']}
- æƒ…ç»ªæ•ˆä»·åå‘ **{valence_desc}**ï¼Œå”¤é†’åº¦å¤„äº **{arousal_desc}** æ°´å¹³
- æƒ…ç»ªåˆ†å¸ƒè¾ƒä¸º{'é›†ä¸­' if float(emotion_data['ç½®ä¿¡åº¦'].replace('%', '')) > 80 else 'åˆ†æ•£'}ï¼Œä¸»è¦æƒ…ç»ªå ä¸»å¯¼åœ°ä½

---

## ğŸ§  èƒ½åŠ›è¯„ä¼°

### IQ ä»£ç†åˆ†æ•°

**åˆ†æ•°**: {ability_data['IQ ä»£ç†åˆ†æ•°']}  
**ç­‰çº§**: {iq_level['level']} ({iq_level['desc']})

### EQ ä»£ç†åˆ†æ•°

**åˆ†æ•°**: {ability_data['EQ ä»£ç†åˆ†æ•°']}  
**ç­‰çº§**: {eq_level['level']} ({eq_level['desc']})

### èƒ½åŠ›è¯„ä¼°è¯´æ˜

- **IQ ä»£ç†åˆ†æ•°**: åŸºäºçœ¼éƒ¨ç‰¹å¾çš„è®¤çŸ¥èƒ½åŠ›ä»£ç†æŒ‡æ ‡
- **EQ ä»£ç†åˆ†æ•°**: åŸºäºçœ¼éƒ¨ç‰¹å¾çš„æƒ…ç»ªæ™ºåŠ›ä»£ç†æŒ‡æ ‡
- åˆ†æ•°èŒƒå›´: 0-100åˆ†
- âš ï¸ **é‡è¦æç¤º**: è¿™äº›åˆ†æ•°ä»…åŸºäºçœ¼éƒ¨å›¾åƒç‰¹å¾ï¼Œæœªç»ç§‘å­¦éªŒè¯ï¼Œä»…ä¾›å‚è€ƒï¼Œä¸åº”ä½œä¸ºå®é™…èƒ½åŠ›è¯„ä¼°çš„ä¾æ®

---

## ğŸ“ˆ æ•°æ®è¯¦æƒ…

### åŸå§‹è¾“å‡ºå€¼

- æ•ˆä»· (Valence): {raw_data['valence']:.4f}
- å”¤é†’åº¦ (Arousal): {raw_data['arousal']:.4f}
- IQ ä»£ç†åŸå§‹å€¼: {raw_data['iq_proxy']:.4f}
- EQ ä»£ç†åŸå§‹å€¼: {raw_data['eq_proxy']:.4f}

---

## ğŸ“ åˆ†æè¯´æ˜

æœ¬æŠ¥å‘ŠåŸºäºæ·±åº¦å­¦ä¹ æ¨¡å‹å¯¹çœ¼éƒ¨å›¾åƒçš„åˆ†æç»“æœç”Ÿæˆã€‚åˆ†æå†…å®¹åŒ…æ‹¬ï¼š

1. **æƒ…ç»ªè¯†åˆ«**: è¯†åˆ«7ç§åŸºæœ¬æƒ…ç»ªï¼ˆæ„¤æ€’ã€åŒæ¶ã€ææƒ§ã€å¿«ä¹ã€ä¸­æ€§ã€æ‚²ä¼¤ã€æƒŠè®¶ï¼‰
2. **æƒ…ç»ªç»´åº¦**: è¯„ä¼°æƒ…ç»ªçš„æ•ˆä»·ï¼ˆç§¯æ/æ¶ˆæï¼‰å’Œå”¤é†’åº¦ï¼ˆé«˜/ä½ï¼‰
3. **èƒ½åŠ›è¯„ä¼°**: æä¾›IQå’ŒEQçš„ä»£ç†åˆ†æ•°ï¼ˆä»…ä¾›å‚è€ƒï¼‰

**å…è´£å£°æ˜**: 
- æœ¬åˆ†æç»“æœä»…åŸºäºçœ¼éƒ¨å›¾åƒç‰¹å¾ï¼Œä¸æ„æˆåŒ»å­¦æˆ–å¿ƒç†å­¦è¯Šæ–­
- IQ/EQåˆ†æ•°ä¸ºä»£ç†æŒ‡æ ‡ï¼Œæœªç»ç§‘å­¦éªŒè¯ï¼Œä»…ä¾›å‚è€ƒ
- å®é™…èƒ½åŠ›è¯„ä¼°åº”é€šè¿‡ä¸“ä¸šçš„æ ‡å‡†åŒ–æµ‹è¯•è¿›è¡Œ

---

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}*
"""
    
    return report


def get_score_level(score: float) -> dict:
    """æ ¹æ®åˆ†æ•°è¿”å›ç­‰çº§æè¿°"""
    if score >= 80:
        return {"level": "ä¼˜ç§€", "desc": "è¡¨ç°ä¼˜å¼‚"}
    elif score >= 70:
        return {"level": "è‰¯å¥½", "desc": "è¡¨ç°è‰¯å¥½"}
    elif score >= 60:
        return {"level": "ä¸­ç­‰", "desc": "è¡¨ç°ä¸­ç­‰"}
    elif score >= 50:
        return {"level": "ä¸€èˆ¬", "desc": "è¡¨ç°ä¸€èˆ¬"}
    else:
        return {"level": "å¾…æå‡", "desc": "æœ‰æå‡ç©ºé—´"}


def main():
    parser = argparse.ArgumentParser(description="ç”Ÿæˆå›¾ç‰‡åˆ†ææŠ¥å‘Š")
    parser.add_argument("--images", type=str, nargs="+", help="å›¾ç‰‡è·¯å¾„åˆ—è¡¨")
    parser.add_argument("--image-dir", type=str, help="å›¾ç‰‡ç›®å½•ï¼ˆæ‰¹é‡å¤„ç†ï¼‰")
    parser.add_argument("--checkpoint", type=str, default="checkpoints/epoch_30.pt", help="æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--output-dir", type=str, default="data/reports", help="æŠ¥å‘Šè¾“å‡ºç›®å½•")
    parser.add_argument("--skip-extraction", action="store_true", help="è·³è¿‡çœ¼éƒ¨æå–")
    parser.add_argument("--device", type=str, default="cuda", help="è®¡ç®—è®¾å¤‡")
    
    args = parser.parse_args()
    
    checkpoint_path = Path(args.checkpoint)
    output_dir = Path(args.output_dir)
    
    # æ”¶é›†å›¾ç‰‡åˆ—è¡¨
    image_paths = []
    if args.images:
        image_paths = [Path(p) for p in args.images]
    elif args.image_dir:
        image_dir = Path(args.image_dir)
        image_paths = list(image_dir.glob("*.jpg")) + list(image_dir.glob("*.png"))
    else:
        # é»˜è®¤ä½¿ç”¨demo_imagesç›®å½•
        image_dir = project_root / "data" / "demo_images"
        image_paths = list(image_dir.glob("*.jpg")) + list(image_dir.glob("*.png"))
    
    if not image_paths:
        print("é”™è¯¯: æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
        return
    
    print(f"æ‰¾åˆ° {len(image_paths)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹ç”ŸæˆæŠ¥å‘Š...\n")
    
    # ç”ŸæˆæŠ¥å‘Š
    reports = []
    for img_path in image_paths:
        try:
            report_path, json_path = generate_report(
                img_path, checkpoint_path, output_dir, 
                args.skip_extraction, args.device
            )
            reports.append((img_path.name, report_path, json_path))
        except Exception as e:
            print(f"âœ— å¤„ç† {img_path.name} æ—¶å‡ºé”™: {e}")
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    if reports:
        summary_path = output_dir / "summary.md"
        generate_summary(reports, summary_path)
        print(f"\nâœ“ æ±‡æ€»æŠ¥å‘Šå·²ç”Ÿæˆ: {summary_path}")
    
    print(f"\nå®Œæˆï¼å…±ç”Ÿæˆ {len(reports)} ä»½æŠ¥å‘Š")


def generate_summary(reports: list, output_path: Path):
    """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
    summary = f"""# æ‰¹é‡åˆ†ææ±‡æ€»æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")}  
**åˆ†æå›¾ç‰‡æ•°é‡**: {len(reports)}

---

## ğŸ“‹ æŠ¥å‘Šåˆ—è¡¨

"""
    
    for i, (img_name, report_path, json_path) in enumerate(reports, 1):
        summary += f"{i}. **{img_name}**\n"
        summary += f"   - MarkdownæŠ¥å‘Š: `{report_path.name}`\n"
        summary += f"   - JSONæ•°æ®: `{json_path.name}`\n\n"
    
    summary += "---\n\n*æ‰€æœ‰æŠ¥å‘Šä¿å­˜åœ¨åŒä¸€ç›®å½•ä¸‹*\n"
    
    output_path.write_text(summary, encoding='utf-8')


if __name__ == "__main__":
    main()

